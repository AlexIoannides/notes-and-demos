{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to JAX\n",
    "\n",
    "[JAX](https://jax.readthedocs.io/en/latest/index.html) is an advanced scientific computing library for Python. At a very crude level, it's an alternative to Numpy that provides auto-differentiation and Xcellerated Linear Algebra (XLA) that can run on GPUs. Unlike Tensorflow and PyTorch, it only operates within a functional programming paradism, where all functions must be pure (enforce referential transparency). It is developed and maintained by Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX as Accelerated Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX implements the Numpy API and more-often-than-not it can be used as a stand-in replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 2., 3., 4., 5.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([1., 2., 3., 4., 5.])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, however, a few subtle differences. Firstly, the output is of type `DeviceArray`, which represents an array on a particular device (CPU, GPU, etc.). To convert this back to raw Numpy, use the `to_py` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 4., 5.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to_py()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, when a JAX function is called, the corresponding operation is dispatched to an accelerator to be computed asynchronously when possible. The returned array is therefore not necessarily ‘filled in’ as soon as the function returns. Thus, if we don’t require the result immediately, the computation won’t block Python execution. If the computation is required - e.g., when using the `block_until_ready` method, converting to a Numpy array or printing a result, then the call to the computation will be blocking. See [Asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#asynchronous-dispatch) in the JAX docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = jnp.dot(x, x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation\n",
    "\n",
    "Auto-differentiation is Jax follows a functional programming paradigm. For example, consider the following example from PyTorch,\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "x.grad\n",
    "\n",
    "# tensor(4.)\n",
    "```\n",
    "\n",
    "In Jax this would be implemented as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(4., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def x_squared(x: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "x_squared_dx = jax.grad(x_squared)\n",
    "x_squared_dx(jnp.array(2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be extended to more complex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.sum_of_squares(x: jaxlib.xla_extension.DeviceArrayBase) -> jaxlib.xla_extension.DeviceArrayBase>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_of_squares(x: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "  return jnp.sum(x**2)\n",
    "\n",
    "\n",
    "sum_of_squares_dx = jax.grad(sum_of_squares)\n",
    "sum_of_squares_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum-of-squares = 55.0\n",
      "d sum-of-squares / dx = [ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"sum-of-squares = {sum_of_squares(x)}\")\n",
    "print(f\"d sum-of-squares / dx = {sum_of_squares_dx(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `grad` operator will only compute derivatives relative to a function's first argument. To extend this to other arguments we use `argnums`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.sum_of_squared_errors(x: jaxlib.xla_extension.DeviceArrayBase, y: jaxlib.xla_extension.DeviceArrayBase) -> jaxlib.xla_extension.DeviceArrayBase>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_of_squared_errors(x: jnp.DeviceArray, y: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "    return jnp.sum((x - y) ** 2)\n",
    "\n",
    "sum_of_squared_errors_dx = jax.grad(sum_of_squared_errors, argnums=(0, 1))\n",
    "sum_of_squared_errors_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum-of-squared-errors = 0.5500001907348633\n",
      "d sum-of-squared-errors / dx = [-0.20000005 -0.4000001  -0.6000004  -0.8000002  -1.        ]\n",
      "d sum-of-squared-errors / dy = [0.20000005 0.4000001  0.6000004  0.8000002  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"sum-of-squared-errors = {sum_of_squared_errors(x, x*1.1)}\")\n",
    "print(f\"d sum-of-squared-errors / dx = {sum_of_squared_errors_dx(x, x*1.1)[0]}\")\n",
    "print(f\"d sum-of-squared-errors / dy = {sum_of_squared_errors_dx(x, x*1.1)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dc4f215462ea912f4965da2670482d3eef22f25452006b3bdce86b7cb4ab1a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
