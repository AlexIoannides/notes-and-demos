{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-married",
   "metadata": {},
   "source": [
    "# Logistic Regression with SGD\n",
    "\n",
    "Exploring how another one of the most basic machine learning models can be implemented using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-telephone",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "emerging-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-orbit",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southeast-police",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105d1e390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-current",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-tampa",
   "metadata": {},
   "source": [
    "Start by creating a dataset and dataloader for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worldwide-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples = 400\n",
      "data[0] = (tensor([-2.]), tensor([0.]))\n",
      "mini_batch[0] = [tensor([[-2.0000],\n",
      "        [-1.9900],\n",
      "        [-1.9800],\n",
      "        [-1.9700],\n",
      "        [-1.9600]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])]\n"
     ]
    }
   ],
   "source": [
    "class LinearlySeperableData(Dataset):\n",
    "    def __init__(self, b: float, w: float, sigma: float = 0.1):\n",
    "        self.w = torch.tensor(w)\n",
    "        self.b = torch.tensor(b)\n",
    "        self.sigma = sigma\n",
    "        self.X = torch.arange(-2, 2, 0.01).view(-1, 1)\n",
    "\n",
    "        z = self.b + self.w * self.X\n",
    "        self.y = torch.where(z + self.sigma * torch.randn(self.X.size()) > 0, 1.0, 0.0)\n",
    "        self.len = self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: float) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        return (self.X[idx], self.y[idx])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.len\n",
    "\n",
    "\n",
    "data = LinearlySeperableData(b=0, w=1)\n",
    "print(f\"n_samples = {len(data)}\")\n",
    "print(f\"data[0] = {data[0]}\")\n",
    "\n",
    "data_loader = DataLoader(dataset=data, batch_size=5)\n",
    "data_batches = list(data_loader)\n",
    "print(f\"mini_batch[0] = {data_batches[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-sydney",
   "metadata": {},
   "source": [
    "## Logistic Regression with the PyTorch Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-diary",
   "metadata": {},
   "source": [
    "Now define the model. We will use a Binary Cross Entropy (BCE) loss function, which is equivalent to the negative of the log-likelhood function for a set of Bernouli trials - see [here](https://en.wikipedia.org/wiki/Cross_entropy#Relation_to_log-likelihood) for for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "agreed-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(torch.nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "        # this is an alternative to torch.sigmoid(torch.nn.Linear())\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 1), torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Compute a prediction.\"\"\"\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-supervisor",
   "metadata": {},
   "source": [
    "Now define the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "functioning-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    n_epochs: int,\n",
    "    learning_rate: float,\n",
    ") -> Sequence[float]:\n",
    "    \"\"\"Train the model over multiple epochs recording the loss for each.\"\"\"\n",
    "\n",
    "    def process_batch(X: torch.FloatTensor, y: torch.FloatTensor) -> float:\n",
    "        y_hat = model.forward(X)\n",
    "        loss = criterion(y_hat, y)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        return loss.detach().numpy().tolist()\n",
    "\n",
    "    def process_epoch() -> float:\n",
    "        return [process_batch(X, y) for X, y in data_loader][-1]\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "    training_run = [process_epoch() for epoch in range(n_epochs)]\n",
    "    return training_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-wichita",
   "metadata": {},
   "source": [
    "We now training the model using `optim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "outer-french",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1400395780801773,\n",
       " 0.03897496312856674,\n",
       " 0.016650056466460228,\n",
       " 0.008784324862062931,\n",
       " 0.005244280211627483,\n",
       " 0.003394478466361761,\n",
       " 0.002325858222320676,\n",
       " 0.001662470051087439,\n",
       " 0.0012277166824787855,\n",
       " 0.0009306239662691951,\n",
       " 0.0007206659065559506,\n",
       " 0.0005679914029315114,\n",
       " 0.0004545174597296864,\n",
       " 0.00036841287510469556,\n",
       " 0.000302050553727895,\n",
       " 0.00025009672390297055,\n",
       " 0.00020888847939204425,\n",
       " 0.00017586114699952304,\n",
       " 0.0001491658331360668,\n",
       " 0.00012727596913464367]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_clf = LogisticRegressionPyTorch(1)\n",
    "loss = torch.nn.BCELoss()\n",
    "train(logistic_clf, loss, data_loader, n_epochs=20, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-sweet",
   "metadata": {},
   "source": [
    "Take a look at estimated parameters. Note how unimportant (unconstrained) the slope parameter is for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coated-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight: [[4.43060827255249]]\n",
      "model.0.bias: [0.2427944391965866]\n"
     ]
    }
   ],
   "source": [
    "for k, v in logistic_clf.state_dict().items():\n",
    "    print(f\"{k}: {v.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-findings",
   "metadata": {},
   "source": [
    "Testing the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-affair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9800)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = LinearlySeperableData(b=0, w=1)\n",
    "\n",
    "y_hat = torch.where(logistic_clf.forward(test_data.X) > 0.5, 1.0, 0.0)\n",
    "accuracy = torch.sum(torch.where(y_hat == test_data.y, 1.0, 0.0)) / len(test_data)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "53195f2e71dcfe3ea716e20379841f9508c7537c854fc223ac4e4b5eed7dc1d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
