{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lifecycle Management with MLflow\n",
    "\n",
    "This notebook is based around a multi-class classification task using the [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). The aim is to demonstrate how [MLflow](https://mlflow.org) can be used to:\n",
    "\n",
    "* track training metrics\n",
    "* manage model persistence\n",
    "* deploy scoring services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "All package imports are declared in this section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tracking Server\n",
    "\n",
    "In order to enable all MLflow features, it is necessary to start a MLflow tracking server backed by a database and a filesystem. To do this, open a shell and execute the following command,\n",
    "\n",
    "```shell\n",
    "mlflow server \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --default-artifact-root mlruns \\\n",
    "    --host 127.0.0.1\n",
    "```\n",
    "\n",
    "Which will create a SQLite database and the `mlruns` directory, both locally, before starting the tracking server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "* set the MLFlow Pyton client to use the local tracking server we've just started.\n",
    "* create an MLFlow `experiment` for tracking run and models associated with our ML task and then set it as the global default for this session.\n",
    "* define a model name constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Iris Classification' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "mlflow.set_experiment('Iris Classification')\n",
    "\n",
    "MODEL_NAME = 'iris_classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Currently hosted as a CSV file on AWS S3 object storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "       species  \n",
       "0       setosa  \n",
       "1       setosa  \n",
       "2       setosa  \n",
       "3       setosa  \n",
       "4       setosa  \n",
       "..         ...  \n",
       "145  virginica  \n",
       "146  virginica  \n",
       "147  virginica  \n",
       "148  virginica  \n",
       "149  virginica  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url = (\n",
    "    'http://bodywork-ml-ops-project'\n",
    "    '.s3.eu-west-2.amazonaws.com/data/iris_classification_data.csv'\n",
    ")\n",
    "data = pd.read_csv(urlopen(data_url))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "* extract class labels\n",
    "* split features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'sepal length (cm)',\n",
    "    'sepal width (cm)',\n",
    "    'petal length (cm)',\n",
    "    'petal width (cm)'\n",
    "]\n",
    "\n",
    "label_column = 'species'\n",
    "species_to_class_map = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
    "\n",
    "X = data[feature_columns].values\n",
    "y = data[label_column].apply(lambda e: species_to_class_map[e]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train and Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function for Logging Model Performance Metrics\n",
    "\n",
    "* compute accuracy and the f1-score\n",
    "* log the metrics to the MLflow tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(y_actual, y_predicted) -> None:\n",
    "    time_now = datetime.now().isoformat(timespec='seconds')\n",
    "    accuracy = balanced_accuracy_score(\n",
    "        y_actual,\n",
    "        y_predicted,\n",
    "        adjusted=True\n",
    "    )\n",
    "    f1 = f1_score(\n",
    "        y_actual,\n",
    "        y_predicted,\n",
    "        average='weighted'\n",
    "    )\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('f1', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "We start by writing a function that will train a decision tree classifier, given a choice of two hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    max_depth: int,\n",
    "    random_state: int\n",
    ") -> DecisionTreeClassifier:\n",
    "    \"\"\"Train a single model, given hyper-parameters.\"\"\"\n",
    "    iris_tree_classifier = DecisionTreeClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=random_state,\n",
    "        max_depth=max_depth\n",
    "    )\n",
    "    iris_tree_classifier.fit(X_train, y_train)\n",
    "    return iris_tree_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Search\n",
    "\n",
    "Within a single MLflow 'parent' training run, we create 'child' runs to encapsulate models trained using randomly generated hyper-paraeters. Each child run logs the parameters used and the associated performance metrics, to MLflow.\n",
    "\n",
    "Once all the models are trained, we search for the best performing set of parameters and use them to train a model on the full dataset, which is then logged to the MLflow model registry and transitioned to `Production`. We also persist some ad hoc model metadata, in this case the feature names and class-name-to-label mapping, as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.72it/s]\n",
      "Successfully registered model 'iris_classifier'.\n",
      "2020/12/23 17:37:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: iris_classifier, version 1\n",
      "Created version '1' of model 'iris_classifier'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='Best Model') as parent_run:\n",
    "    for _ in tqdm(range(10)):\n",
    "        with mlflow.start_run(run_name='Candidate Run', nested=True) as child_run:\n",
    "            max_depth = random.randint(1, 4)\n",
    "            random_state = random.randint(1, 100)\n",
    "            mlflow.log_param('random_state', random_state)\n",
    "            mlflow.log_param('max_depth', max_depth)\n",
    "            trained_model = train_model(X_train, y_train, max_depth, random_state)\n",
    "            log_metrics(y_test, trained_model.predict(X_test))\n",
    "\n",
    "    # get best model parameters\n",
    "    best_run = (\n",
    "        mlflow.search_runs(parent_run.info.experiment_id)\n",
    "        .sort_values(by=['metrics.f1', 'metrics.accuracy'], ascending=False)\n",
    "        [:1]\n",
    "    )\n",
    "    best_f1 = float(best_run['metrics.f1'])\n",
    "    best_accuracy = float(best_run['metrics.accuracy'])\n",
    "    best_max_depth = int(best_run['params.max_depth'])\n",
    "    best_random_state = int(best_run['params.random_state'])\n",
    "    best_model = train_model(X, y, best_max_depth, best_random_state)\n",
    "\n",
    "    # log best model parameters\n",
    "    mlflow.log_param('best_max_depth', best_max_depth)\n",
    "    mlflow.log_param('best_random_state', best_random_state)\n",
    "    mlflow.log_metric('best_f1', best_f1)\n",
    "    mlflow.log_metric('best_accuracy', best_accuracy)\n",
    "    mlflow.set_tag('model_estimated_on_full_dataset', \"true\")\n",
    "\n",
    "    # persist additional metadata\n",
    "    with open('features.txt', 'w') as f:\n",
    "        f.write(', '.join(feature_columns))\n",
    "    mlflow.log_artifact('features.txt')    \n",
    "    with open('class_labels.txt', 'w') as f:\n",
    "        f.write(', '.join(f'{k}: {v}' for k, v in species_to_class_map.items()))\n",
    "    mlflow.log_artifact('class_labels.txt')\n",
    "\n",
    "    # train final model using best parameters on full dataset\n",
    "    mlflow.sklearn.log_model(sk_model=best_model, artifact_path=MODEL_NAME)\n",
    "    new_model_metadata = mlflow.register_model(\n",
    "        model_uri=f'runs:/{parent_run.info.run_id}/{MODEL_NAME}',\n",
    "        name=MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # push new model to production\n",
    "    mlflow.tracking.MlflowClient().transition_model_version_stage(\n",
    "        name=MODEL_NAME,\n",
    "        version=int(new_model_metadata.version),\n",
    "        stage='Production'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Model from Registry\n",
    "\n",
    "We test that the best model found in our training run has been correctly persisted to the MLflow model registry, by loading the latest version available from `Production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', max_depth=3, random_state=35)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.sklearn.load_model(model_uri=f'models:/{MODEL_NAME}/Production')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve New Model\n",
    "\n",
    "We would like to make the model available as a scoring service with a REST API. Assuming that there is a Conda-copatible version of Python installed locally, then this can be achieved from a new shell using the command below,\n",
    "\n",
    "```shell\n",
    "MLFLOW_TRACKING_URI=\"http://127.0.0.1:5000\" \\\n",
    "MLFLOW_CONDA_HOME=\"/Users/alexioannides/opt/anaconda3\" \\\n",
    "mlflow models serve -m \"models:/iris_classifier/Production\" -\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model-Scoring Service\n",
    "\n",
    "To test the model scoring service started above, open yet another new shell and use the curl tool to issue a HTTP request,\n",
    "\n",
    "```shell\n",
    "curl http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d '{\n",
    "    \"columns\": [\"a\", \"b\", \"c\", \"d\"],\n",
    "    \"data\": [[5.1, 3.5, 1.4, 0.2]]\n",
    "}'\n",
    "```\n",
    "\n",
    "Which will return the predicted class label from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-Up\n",
    "\n",
    "Stop the scoring service and model tracking server and then delete:\n",
    "\n",
    "* the SQLite database used by the tracking server.\n",
    "* the local directory used by the tracking server to persist models and artefacts.\n",
    "* all temporary artefact files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('mlflow.db')\n",
    "os.remove('features.txt')\n",
    "os.remove('class_labels.txt')\n",
    "shutil.rmtree('mlruns', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
